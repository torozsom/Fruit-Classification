{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Imports and environment setup\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    total_mem_gb = props.total_memory / (1024**3)\n",
    "\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total memory: {total_mem_gb:.1f} GB\")\n",
    "    print(f\"SM count: {props.multi_processor_count}\")\n",
    "    print(f\"Compute Capability (SM): {props.major}.{props.minor}\")\n",
    "\n",
    "    # ================================\n",
    "    #  PyTorch optimization\n",
    "    # ================================\n",
    "\n",
    "    # 1. TF32 acceleration\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 2. cuDNN optimization (choose the fastest kernel)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "else:\n",
    "    print(\"Not CUDA compatible, CPU will be used.\")\n"
   ],
   "id": "d428f5de760607a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset: Kaggle fruit detection dataset\n",
    "# The dataset contains annotations (_annotations.csv), here we classify pictures so that\n",
    "# a picture's label will be its most frequently occuring object class.\n",
    "\n",
    "class FruitsFromAnnotations(Dataset):\n",
    "    def __init__(self, images_dir: str, annotations_csv: str, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_csv = annotations_csv\n",
    "        self.transform = transform\n",
    "\n",
    "        # Reading annotations\n",
    "        df = pd.read_csv(annotations_csv)\n",
    "        # Choose the most frequent class for each image (filename)\n",
    "        agg = (\n",
    "            df.groupby('filename')['class']\n",
    "              .agg(lambda s: s.value_counts().index[0])\n",
    "              .reset_index()\n",
    "        )\n",
    "\n",
    "        # Gather class names and their indices alphabetically\n",
    "        classes = sorted(agg['class'].unique().tolist())\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
    "\n",
    "        # List of (image path, class index) tuples\n",
    "        self.samples = []\n",
    "        for _, row in agg.iterrows():\n",
    "            img_path = os.path.join(images_dir, row['filename'])\n",
    "            if os.path.exists(img_path):\n",
    "                self.samples.append((img_path, self.class_to_idx[row['class']]))\n",
    "\n",
    "        print(f\"Loaded samples: {len(self.samples)} | Classes: {len(self.class_to_idx)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ],
   "id": "1e7b0e834b42db00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ],
   "id": "856846f23fab6332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Datasets, dataloaders\n",
    "ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '.')) if '__file__' in globals() else os.getcwd()\n",
    "train_images = os.path.join(ROOT, 'archive', 'train')\n",
    "valid_images = os.path.join(ROOT, 'archive', 'valid')\n",
    "test_images = os.path.join(ROOT, 'archive', 'test')\n",
    "\n",
    "train_csv = os.path.join(train_images, '_annotations.csv')\n",
    "valid_csv = os.path.join(valid_images, '_annotations.csv')\n",
    "test_csv = os.path.join(test_images, '_annotations.csv') if os.path.exists(os.path.join(test_images, '_annotations.csv')) else None\n",
    "\n",
    "train_ds = FruitsFromAnnotations(train_images, train_csv, transform=transform_train)\n",
    "valid_ds = FruitsFromAnnotations(valid_images, valid_csv, transform=transform_eval)\n",
    "\n",
    "num_classes = len(train_ds.class_to_idx)\n",
    "assert num_classes == len(valid_ds.class_to_idx), \"Train and valid class suite differs.\"\n",
    "\n",
    "batch_size = 64\n",
    "pin = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Valid batches: {len(valid_loader)}\")\n"
   ],
   "id": "f4f19ccecf83c55d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up model (MobileNetV2, pretrained)\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "in_features = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ],
   "id": "935e597daf49c6d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training loop\n",
    "num_epochs = 6\n",
    "train_losses, valid_losses = [], []\n",
    "train_accs, valid_accs = [], []\n",
    "\n",
    "all_valid_preds = []\n",
    "all_valid_labels = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f\"  Train batch {batch_idx+1}/{len(train_loader)} | loss={loss.item():.4f}\")\n",
    "\n",
    "    epoch_train_loss = running_loss / max(1, len(train_loader))\n",
    "    epoch_train_acc = 100.0 * running_correct / max(1, running_total)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accs.append(epoch_train_acc)\n",
    "\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    v_loss, v_correct, v_total = 0.0, 0, 0\n",
    "    epoch_preds, epoch_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            v_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            v_correct += (preds == labels).sum().item()\n",
    "            v_total += labels.size(0)\n",
    "            epoch_preds.extend(preds.cpu().numpy().tolist())\n",
    "            epoch_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    epoch_valid_loss = v_loss / max(1, len(valid_loader))\n",
    "    epoch_valid_acc = 100.0 * v_correct / max(1, v_total)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    valid_accs.append(epoch_valid_acc)\n",
    "\n",
    "    all_valid_preds = epoch_preds  # Epoch results\n",
    "    all_valid_labels = epoch_labels\n",
    "\n",
    "    print(f\"Epoch ended | train_loss={epoch_train_loss:.4f}, train_acc={epoch_train_acc:.2f}% | \"\n",
    "          f\"valid_loss={epoch_valid_loss:.4f}, valid_acc={epoch_valid_acc:.2f}%\")\n"
   ],
   "id": "a4ba98828496cd6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualizing learning curves\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(valid_losses, marker='s', label='Valid Loss')\n",
    "plt.title('Loss curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, marker='o', label='Train Acc')\n",
    "plt.plot(valid_accs, marker='s', label='Valid Acc')\n",
    "plt.title('Accuracy curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "try:\n",
    "    os.makedirs('GeneratedPhotos', exist_ok=True)\n",
    "    plt.savefig('./GeneratedPhotos/learning_curves.png', dpi=150)\n",
    "    print(\"Learning curves saved: learning_curves.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Saving picture failed: {e}\")\n",
    "plt.show()\n"
   ],
   "id": "57541c043956019c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation: Confusion Matrix, Precision, Recall, F1\n",
    "class_names = [train_ds.idx_to_class[i] for i in range(num_classes)]\n",
    "\n",
    "assert len(all_valid_labels) == len(all_valid_preds), \"The length of labels and predictions must be equal.\"\n",
    "if len(all_valid_labels) == 0:\n",
    "    print(\"Warning: No valid samples for evaluation.\")\n",
    "else:\n",
    "    cm = confusion_matrix(all_valid_labels, all_valid_preds, labels=list(range(num_classes)))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix (Valid) - Counts')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        plt.savefig('./GeneratedPhotos/confusion_matrix_counts.png', dpi=150)\n",
    "        print(\"Confusion matrix (counts) saved: confusion_matrix_counts.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"CM save failed: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Normalized CM (row by row)\n",
    "    cm_norm = confusion_matrix(all_valid_labels, all_valid_preds, labels=list(range(num_classes)), normalize='true')\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens', xticklabels=class_names, yticklabels=class_names, vmin=0, vmax=1)\n",
    "    plt.title('Confusion Matrix (Valid) – Normalized')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        plt.savefig('./GeneratedPhotos/confusion_matrix_normalized.png', dpi=150)\n",
    "        print(\"Confusion matrix (normalized) saved: confusion_matrix_normalized.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"CM norm save failed: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "if len(all_valid_labels) > 0:\n",
    "    prec = precision_score(all_valid_labels, all_valid_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(all_valid_labels, all_valid_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_valid_labels, all_valid_preds, average='macro', zero_division=0)\n",
    "    print(f\"Precision (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro):    {rec:.4f}\")\n",
    "    print(f\"F1-score (macro):  {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nDetailed report:\\n\")\n",
    "    print(classification_report(all_valid_labels, all_valid_preds, target_names=class_names, zero_division=0))\n",
    "else:\n",
    "    print(\"The metrics cannot be calculated because there are no valid predictions.\")\n",
    "\n"
   ],
   "id": "38aed4a0953e3c6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Some valid samples visualized with predictions\n",
    "def imshow_tensor(img_tensor):\n",
    "    inv_norm = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    img = inv_norm(img_tensor.cpu()).clamp(0,1)\n",
    "    npimg = img.permute(1,2,0).numpy()\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis('off')\n",
    "\n",
    "model.eval()\n",
    "images_shown = 0\n",
    "plt.figure(figsize=(12,8))\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1).cpu()\n",
    "        for i in range(min(8, images.size(0))):\n",
    "            plt.subplot(2,4,images_shown+1)\n",
    "            imshow_tensor(images[i])\n",
    "            plt.title(f\"Pred: {class_names[preds[i].item()]}\")\n",
    "            images_shown += 1\n",
    "            if images_shown == 8:\n",
    "                break\n",
    "        break\n",
    "plt.tight_layout()\n",
    "try:\n",
    "    plt.savefig('./GeneratedPhotos/val_samples.png', dpi=150)\n",
    "    print(\"Samples saved: val_samples.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Sample save failed: {e}\")\n",
    "plt.show()\n"
   ],
   "id": "742d5fc70b8cc3d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quantization Aware Training (QAT) with Pytorch\n",
    "\n",
    "This optional section demonstrates how to run Quantization Aware Training (QAT) on the trained MobileNetV2 model using the FX graph mode quantization APIs in PyTorch. It will:\n",
    "\n",
    "- Prepare a CPU copy of the trained model for QAT.\n",
    "- Optionally perform a short fine-tuning loop for QAT on a small subset of the training data (to keep it fast).\n",
    "- Convert the model to a quantized int8 model and evaluate accuracy on the validation set.\n",
    "- Compare model size and a small inference speed sample.\n",
    "\n",
    "Notes:\n",
    "- QAT fine-tuning runs on CUDA if available (it uses fake-quant modules that work on GPU). INT8 conversion and inference remain on CPU (x86 fbgemm kernels).\n",
    "- Deprecation/User warnings from legacy torch.ao.quantization APIs are suppressed here for cleaner output; consider migrating to torchao pt2e (prepare_pt2e/convert_pt2e) in the future.\n",
    "- To keep your main workflow unaffected, this block is guarded by RUN_QAT flag (default False).\n",
    "\n"
   ],
   "id": "a547a1e577323330"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:32:05.981790Z",
     "start_time": "2025-11-22T15:30:35.564260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import torch.ao.quantization as tq\n",
    "    from torch.ao.quantization import get_default_qat_qconfig, QConfigMapping\n",
    "    # Compatibility: in some PyTorch versions the FX APIs live in quantize_fx, in others in fx\n",
    "    try:\n",
    "        from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx  # PyTorch >= 1.13/2.x\n",
    "    except Exception:\n",
    "        try:\n",
    "            from torch.ao.quantization.fx import prepare_qat_fx, convert_fx  # Older layout\n",
    "        except Exception as _qat_import_err:\n",
    "            raise _qat_import_err\n",
    "except Exception as _qat_import_err:\n",
    "    print(\n",
    "        \"Warning: QAT imports failed. FX QAT APIs not found in your torch build. \"\n",
    "        f\"Details: {_qat_import_err}\\n\"\n",
    "        \"Tip: Ensure you're using a PyTorch version that provides torch.ao.quantization.quantize_fx or fx.\"\n",
    "    )\n",
    "    tq = None\n",
    "\n",
    "\n",
    "# Guard to avoid running QAT unless explicitly enabled\n",
    "RUN_QAT = True  # Set to True to run a brief QAT fine-tune and convert\n",
    "\n",
    "\n",
    "def evaluate_top1(model_cpu, data_loader):\n",
    "    model_cpu.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to('cpu', non_blocking=True)\n",
    "            labels = labels.to('cpu', non_blocking=True)\n",
    "            outputs = model_cpu(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / max(1, total)\n",
    "\n",
    "\n",
    "def estimate_inference_time(model_cpu, data_loader, warmup=3, iters=10):\n",
    "    model_cpu.eval()\n",
    "    images_it = None\n",
    "    # Get one batch\n",
    "    for images, _ in data_loader:\n",
    "        images_it = images.to('cpu', non_blocking=True)\n",
    "        break\n",
    "    if images_it is None:\n",
    "        return None\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model_cpu(images_it)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iters):\n",
    "            _ = model_cpu(images_it)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_ms = (end - start) * 1000.0 / max(1, iters)\n",
    "    return avg_ms\n",
    "\n",
    "\n",
    "# Only proceed if imports are available\n",
    "if tq is None:\n",
    "    print(\"QAT section skipped because torch.ao.quantization is unavailable.\")\n",
    "else:\n",
    "    # Suppress known deprecation and observer warnings from legacy torch.ao.quantization APIs\n",
    "    try:\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            category=DeprecationWarning,\n",
    "            message=r\".*torch\\.ao\\.quantization is deprecated and will be removed in 2\\.10.*\",\n",
    "        )\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            category=UserWarning,\n",
    "            message=r\".*reduce_range will be deprecated.*\",\n",
    "            module=r\"torch\\.ao\\.quantization\\.observer\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Create a CPU float copy of the trained model\n",
    "    float_model_cpu = copy.deepcopy(model).to('cpu')\n",
    "    float_model_cpu.eval()\n",
    "\n",
    "    # Baseline (float) accuracy and timing\n",
    "    try:\n",
    "        baseline_acc = evaluate_top1(float_model_cpu, valid_loader)\n",
    "        baseline_time_ms = estimate_inference_time(float_model_cpu, valid_loader)\n",
    "        print(f\"[Baseline FP32] Valid top-1 acc: {baseline_acc:.2f}% | Sample avg latency: {baseline_time_ms:.2f} ms/batch\" if baseline_time_ms is not None else f\"[Baseline FP32] Valid top-1 acc: {baseline_acc:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Baseline evaluation failed: {e}\")\n",
    "        baseline_acc, baseline_time_ms = None, None\n",
    "\n",
    "    # QAT preparation (FX graph mode)\n",
    "    example_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "    qconfig = get_default_qat_qconfig('fbgemm')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "    try:\n",
    "        prepared_qat = prepare_qat_fx(float_model_cpu, qconfig_mapping, example_inputs)\n",
    "        print(\"Model prepared for QAT (FX).\")\n",
    "    except Exception as e:\n",
    "        print(f\"QAT prepare failed: {e}\")\n",
    "        prepared_qat = None\n",
    "\n",
    "    quantized_model = None\n",
    "    if prepared_qat is not None:\n",
    "        if RUN_QAT:\n",
    "            # Brief QAT fine-tuning loop on a small subset\n",
    "            qat_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            prepared_qat = prepared_qat.to(qat_device)\n",
    "            criterion_q = nn.CrossEntropyLoss()\n",
    "            optimizer_q = optim.Adam(prepared_qat.parameters(), lr=5e-5)\n",
    "            prepared_qat.train()\n",
    "\n",
    "            max_steps = 50  # keep it small and fast\n",
    "            step_count = 0\n",
    "            print(f\"Starting brief QAT fine-tuning on {qat_device}...\")\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(qat_device, non_blocking=True)\n",
    "                labels = labels.to(qat_device, non_blocking=True)\n",
    "                optimizer_q.zero_grad()\n",
    "                outputs = prepared_qat(images)\n",
    "                loss = criterion_q(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer_q.step()\n",
    "                step_count += 1\n",
    "                if step_count % 10 == 0:\n",
    "                    print(f\"  QAT step {step_count} | loss={loss.item():.4f}\")\n",
    "                if step_count >= max_steps:\n",
    "                    break\n",
    "            print(\"QAT fine-tuning finished.\")\n",
    "        else:\n",
    "            print(\"RUN_QAT is False → skipping QAT fine-tune (you can enable it to improve int8 accuracy).\")\n",
    "\n",
    "        # Convert to quantized model\n",
    "        try:\n",
    "            # Ensure model is on CPU for int8 conversion/inference\n",
    "            prepared_qat = prepared_qat.to('cpu')\n",
    "            quantized_model = convert_fx(prepared_qat)\n",
    "            quantized_model.eval()\n",
    "            print(\"Converted to quantized int8 model.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Quantized convert failed: {e}\")\n",
    "\n",
    "    if quantized_model is not None:\n",
    "        try:\n",
    "            q_acc = evaluate_top1(quantized_model, valid_loader)\n",
    "            q_time_ms = estimate_inference_time(quantized_model, valid_loader)\n",
    "            print(f\"[Quantized INT8] Valid top-1 acc: {q_acc:.2f}% | Sample avg latency: {q_time_ms:.2f} ms/batch\" if q_time_ms is not None else f\"[Quantized INT8] Valid top-1 acc: {q_acc:.2f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Quantized evaluation failed: {e}\")\n",
    "\n",
    "        # Size comparison\n",
    "        try:\n",
    "            # Ensure QAT models directory exists\n",
    "            base_root = ROOT if 'ROOT' in globals() else os.getcwd()\n",
    "            qat_dir = os.path.join(base_root, 'QATmodels')\n",
    "            os.makedirs(qat_dir, exist_ok=True)\n",
    "\n",
    "            fp32_path = os.path.join(qat_dir, 'model_fp32_state_dict.pth')\n",
    "            int8_path = os.path.join(qat_dir, 'model_int8_qat_fx_state_dict.pth')\n",
    "            torch.save(float_model_cpu.state_dict(), fp32_path)\n",
    "            torch.save(quantized_model.state_dict(), int8_path)\n",
    "            fp32_size = os.path.getsize(fp32_path) / (1024**2)\n",
    "            int8_size = os.path.getsize(int8_path) / (1024**2)\n",
    "            print(f\"Saved FP32 state_dict: {fp32_path} ({fp32_size:.2f} MB)\")\n",
    "            print(f\"Saved INT8 state_dict: {int8_path} ({int8_size:.2f} MB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Saving state_dicts failed: {e}\")\n",
    "\n",
    "        # Optional: save a TorchScript version (may fail depending on ops)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\n",
    "                    \"ignore\",\n",
    "                    category=UserWarning,\n",
    "                    message=r\"The TorchScript type system doesn't support instance-level annotations.*\",\n",
    "                    module=r\"torch\\.jit\\._check\"\n",
    "                )\n",
    "                scripted = torch.jit.script(quantized_model)\n",
    "            base_root = ROOT if 'ROOT' in globals() else os.getcwd()\n",
    "            qat_dir = os.path.join(base_root, 'QATmodels')\n",
    "            os.makedirs(qat_dir, exist_ok=True)\n",
    "            ts_path = os.path.join(qat_dir, 'model_int8_qat_fx_scripted.pt')\n",
    "            scripted.save(ts_path)\n",
    "            print(f\"Saved scripted INT8 model: {ts_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Saving scripted INT8 model failed: {e}\")\n"
   ],
   "id": "5ee01400b2833bad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline FP32] Valid top-1 acc: 79.83% | Sample avg latency: 438.30 ms/batch\n",
      "Model prepared for QAT (FX).\n",
      "Starting brief QAT fine-tuning on cuda...\n",
      "  QAT step 10 | loss=1.0398\n",
      "  QAT step 20 | loss=1.0956\n",
      "  QAT step 30 | loss=0.8959\n",
      "  QAT step 40 | loss=0.7308\n",
      "  QAT step 50 | loss=0.5603\n",
      "QAT fine-tuning finished.\n",
      "Converted to quantized int8 model.\n",
      "[Quantized INT8] Valid top-1 acc: 74.33% | Sample avg latency: 97.98 ms/batch\n",
      "Saved FP32 state_dict: C:\\Dev\\AI\\Fruit-Classification\\QATmodels\\model_fp32_state_dict.pth (8.79 MB)\n",
      "Saved INT8 state_dict: C:\\Dev\\AI\\Fruit-Classification\\QATmodels\\model_int8_qat_fx_state_dict.pth (2.54 MB)\n",
      "Saved scripted INT8 model: C:\\Dev\\AI\\Fruit-Classification\\QATmodels\\model_int8_qat_fx_scripted.pt\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
